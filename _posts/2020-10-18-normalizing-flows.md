---
layout: post
title: Normalizing flows
date_event: 2020-10-21T10:20:00Z
tag: upcoming
image: "/images/illustration/event5-flows.png"
permalink: /event-5/
---

Normalizing flows model complex distributions based on a series of bijective functions from a simple base distribution. They come in very handy as they provide us with an invertible transformation that enables densities evaluation and samples generation. \\
However, to be tractable in practice, those functions need to be computed easily, leading to additional assumptions and raising general questions: Which additional assumptions should we use to compute those functions? Can we model in theory and in practice any complex distributions? How can we exploit the bijectivity of the transformation to solve some machine learning problems?  

  - [Dimitris Kalatzis](https://orbit.dtu.dk/en/persons/dimitrios-kalatzis) is a PhD student at the Technical University of Denmark. He is working on generative models and geometric machine learning. He will first present the theory behind the Normalizing flows before giving us a geometric perspective.
  - [Polina Kirichenko](https://polkirichenko.github.io/) is a PhD student at New York University Center for Data Science. Her main interests lie in probabilistic deep learning, uncertainty estimation and generative models. She will present two of her latest papers on Normalizing flows: [Why Normalizing Flows Fail to Detect Out-of-Distribution Data](https://arxiv.org/abs/2006.08545), and [Semi-Supervised Learning with Normalizing Flows](https://arxiv.org/abs/1912.13025).
  - [Antoine Wehenkel](https://awehenkel.github.io/) is a PhD student at University of Li√®ge in Belgium. His main research interests revolve around statistics, machine learning and information theory. He will revisit normalising flows as probabilistic graphical models, and will present his previous work: [You say Normalizing Flows I see Bayesian Networks](https://arxiv.org/abs/2006.00866).
